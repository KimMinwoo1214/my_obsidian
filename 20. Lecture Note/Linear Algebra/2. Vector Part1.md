---
tags:
  - LinearAlgebra
  - MATLAB
aliases: 
type: lecture-note
created: 2024-03-08
reviewed: false
notices: false
summary: true
---
### **Subject**:: [[Linear Algebra(MATLAB)]]
### **Week**:: 1

**ì´ì „ ê°•ì˜ ë…¸íŠ¸**: [[1. Introduce]]

### ê°•ì˜ìë£Œ: 
![[Lecture_02_Vectors_part.pdf]]


# ê°•ì˜í•„ê¸° (Lecture Notes)

- **[[Vector]]**:
    
    - **Definition**: ìˆ«ìë‚˜ ê¸°í˜¸ë¥¼ ì¼ì°¨ì› ë°°ì—´ë¡œ í‘œí˜„í•œ ê²ƒ
    - **Notation**: Bold lowercase letters (e.g., **v**), italicized (v), or with an arrow above (ğ¯).
    - **Characteristics**: ì°¨ì›ê³¼ ë°©í–¥ì„ í¬í•¨í•œë‹¤. (column or row).
    $$
    x = \begin{bmatrix}
    1 \\
    4 \\
    5 \\
    6 \\
    \end{bmatrix}
    \text{, } y = \begin{bmatrix}
    .3 \\
    -7 \\
    \end{bmatrix}
    \text{, } z = \begin{bmatrix}
    1 & 4 & 5 & 6 \\
    \end{bmatrix}
    $$
    $$
    \text{x is a 4-dimensional column vector}
    $$
    $$
    \text{y is a 2-dimensional column vector}
    $$
    $$
    \text{z is a 4-dimensional row vector}
    $$
    - **[[Transpose]]**: í–‰ ë²¡í„°ë¥¼ ì—´ ë²¡í„°ë¡œ, ë˜ëŠ” ì—´ ë²¡í„°ë¥¼ í–‰ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ë°©í–¥ì„ ë’¤ì§‘ëŠ” ê²ƒ
	$$
	\begin{bmatrix}
	x_1 \\
	x_2 \\
	\vdots \\
	x_n
	\end{bmatrix}^T
	=
	\begin{bmatrix}
	x_1 & x_2 & \cdots & x_n
	\end{bmatrix},
	\quad
	\begin{bmatrix}
	x_1 & x_2 & \cdots & x_n
	\end{bmatrix}^T
	=
	\begin{bmatrix}
	x_1 \\
	x_2 \\
	\vdots \\
	x_n
	\end{bmatrix}
	$$
	$$
	\text{Transpose of coloum vector, Transpose of row vector}
	$$
- **[[Vector Operations]]**:
    
    - **Addition and Subtraction**: ê°™ì€ ì°¨ì›ì˜ ë²¡í„°ë“¤ ê°„ì— ê°€ëŠ¥í•˜ë‹¤.
    $$
    \begin{bmatrix}
    4 \\
    5 \\
    6 \\
    \end{bmatrix} + 
    \begin{bmatrix}
    10 \\
    20 \\
    30 \\
    \end{bmatrix}
    =
    \begin{bmatrix}
    14 \\
    25 \\
    36 \\
    \end{bmatrix} \text{, }
    \begin{bmatrix}
    4 \\
    5 \\
    6 \\
    \end{bmatrix} -
    \begin{bmatrix}
    10 \\
    20 \\
    30 \\
    \end{bmatrix}
    =
    \begin{bmatrix}
    -6 \\
    -15 \\
    -24 \\
    \end{bmatrix}
    $$
    - **Scalar-Vector Multiplication**: ë²¡í„°ì˜ ë°©í–¥ì„ ìœ ì§€í•˜ë©´ì„œ í¬ê¸°ë¥¼ ì¡°ì ˆí•œë‹¤.
    $$
    \lambda = 4 \text{, } w = \begin{bmatrix}
    9 \\
    4 \\
    1 \\
    \end{bmatrix} \text{, } \lambda w = \begin{bmatrix}
    36\\
    16\\
    4 \\
    \end{bmatrix}
    $$
    $$
    \lambda: \text{ scalar, w: vector}
    $$
    - **Vector [[Norm]]**:
        - **[[Manhattan]] [[Norm]] (L1 Norm)**:ë²¡í„° ì„±ë¶„ì˜ ì ˆëŒ€ê°’ì˜ í•©
        $$
        ||v||_1 = \Sigma^n_i|x_i| = |x_1| + |x_2| + \cdots + |x_n|
        $$
        - **[[Euclidean]] [[Norm]] (L2 Norm)**: ë²¡í„° ì„±ë¶„ ì œê³±ì˜ í•©ì˜ ì œê³±ê·¼
        $$
        ||v||_2 = \sqrt(\Sigma^n_i(x_i^2)) = \sqrt(x_1^2 + x_2^2 + x_3^2 + \cdots + x_i^2)
        $$
- **[[Dot Product]]**:
    
    - **Definition**: ë‘ ë²¡í„°ì˜ ëŒ€ì‘ë˜ëŠ” ì„±ë¶„ì„ ê³±í•˜ê³  ê·¸ê²ƒë“¤ì„ ëª¨ë‘ ë”í•˜ëŠ” **[[Scalar]]**ê°’
    $$
    \delta = \sum_{i=1}^{n} a_i b_i
    $$
    $$
    u \cdot v = u^T v
    $$
    - **Properties**: ë‘ ë²¡í„° ê°„ì˜ ìœ ì‚¬ì„±ì´ë‚˜ ë§¤í•‘ì„ ì¸¡ì •í•œë‹¤.
    $$
    a \cdot (b+c) = a^T(b+c) = a^Tb + a^Tc
    $$
    - **Geometric Interpretation**: ë²¡í„° ê°„ì˜ ê°ë„ì™€ ê´€ë ¨ì´ ìˆê³ , íŠ¹ë³„í•œ ê²½ìš°ë“¤ë„ í¬í•¨í•œë‹¤.
    $$
    \alpha = cos(\theta_{v,w})\| \mathbf{v} \| \| \mathbf{w} \|
    $$
- **[[Cross Product]]**:
    
    - **Definition**: The vector cross product in â„3 space results in a vector orthogonal to the two input vectors.
    $$
    x \times y = (x_2y_3 - x_3y_2, x_3y_1 - x_1y_3, x_1y_2 - x_2y_1)
    $$
    - **Properties**: Includes distributive, anti-commutative, and scalar multiplication rules.
	    $$
	    1 \text{. }x \times y = -y \times x
	    $$
	    $$
	    2 \text{. } x \times (y + z) = (x \times y) + (x \times z)
	    $$
	    $$
	    3 \text{. } (x + y) \times z = (x \times z) + (y \times z)
	    $$
	    $$
	    4 \text{. } c(x \times y) = (cx) \times y = x \times (cy)
		$$
		$$
	    5 \text{. } x \times 0 = 0 \times x = 0
	    $$
	     $$
	    6 \text{. } x \times x = 0
	    $$
    
- **[[Hadamard Product]]**
	![[hadamard_prodect.png]]
    - **Definition**: Element-wise multiplication of two vectors or matrices of the same dimension.
    - **Notation**: Denoted by âŠ™.
- **[[Orthogonal]] [[Vector]] [[Decomposition]]**:
    
    - **Concept**: Decomposing a vector into components parallel and orthogonal to a reference vector.
    - **Applications**: Relevant in the Gram-Schmidt process and QR decomposition.

## ê³µì§€ì‚¬í•­
<br>



## ìˆ˜ì—… ë‚´ìš©
1. [[Vector]]
2. [[Vector]] [[Operations]]
3. [[Dot Product]]
4. [[Cross Product]]
5. [[Hadamard Product]]
6. [[Orthogonal]] [[Vector]] [[Decomposition]]

# ë‹¤ìŒ í•  ì¼(After Actions)
## ì‘ì—… (Tasks)

### Exercises:
1. Create visualizations for vectors and their operations using Matlab.
2. Implement functions to calculate vector norms, dot products, and perform vector decomposition.
3. Write code for converting vectors between row and column orientations without using built-in functions.

## ì •ë¦¬ (Summary)
[[Matlab]]
- **[[Summary]] of Key Points**:
    
    - **Vector Arithmetic**: Operations are performed element-wise.
    - **Dot Product**: Encodes the relationship between two vectors as a single number.
    - **Orthogonality**: Two vectors are orthogonal if their dot product is zero.
    - **Decomposition**: Involves dividing a vector into parallel and orthogonal components with respect to a reference vector.


