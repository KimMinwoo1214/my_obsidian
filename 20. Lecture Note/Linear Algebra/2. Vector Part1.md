---
tags:
  - LinearAlgebra
  - MATLAB
aliases: 
type: lecture-note
created: 2024-03-08
reviewed: false
notices: false
summary: true
---
### **Subject**:: [[Linear Algebra(MATLAB)]]
### **Week**:: 1

**Ïù¥Ï†Ñ Í∞ïÏùò ÎÖ∏Ìä∏**: [[1. Introduce]]

### Í∞ïÏùòÏûêÎ£å: 
![[Lecture_02_Vectors_part.pdf]]


# Í∞ïÏùòÌïÑÍ∏∞ (Lecture Notes)

- **[[Vector]]**:
    
    - **Definition**: Representations of numbers or symbols in a one-dimensional array.
    - **Notation**: Bold lowercase letters (e.g., **v**), italicized (v), or with an arrow above (ùêØ).
    - **Characteristics**: Include dimensionality and orientation (column or row).
    $$
    x = \begin{bmatrix}
    1 \\
    4 \\
    5 \\
    6 \\
    \end{bmatrix}
    \text{, } y = \begin{bmatrix}
    .3 \\
    -7 \\
    \end{bmatrix}
    \text{, } z = \begin{bmatrix}
    1 & 4 & 5 & 6 \\
    \end{bmatrix}
    $$
    $$
    \text{x is a 4-dimensional column vector}
    $$
    $$
    \text{y is a 2-dimensional column vector}
    $$
    $$
    \text{z is a 4-dimensional row vector}
    $$
    - **Transpose**: Convert row vector to column vector or vice versa, effectively flipping its orientation
	$$
	\begin{bmatrix}
	x_1 \\
	x_2 \\
	\vdots \\
	x_n
	\end{bmatrix}^T
	=
	\begin{bmatrix}
	x_1 & x_2 & \cdots & x_n
	\end{bmatrix},
	\quad
	\begin{bmatrix}
	x_1 & x_2 & \cdots & x_n
	\end{bmatrix}^T
	=
	\begin{bmatrix}
	x_1 \\
	x_2 \\
	\vdots \\
	x_n
	\end{bmatrix}
	$$
	$$
	\text{Transpose of coloum vector, Transpose of row vector}
	$$
- **[[Vector Operations]]**:
    
    - **Addition and Subtraction**: Possible between vectors of the same dimension.
    $$
    \begin{bmatrix}
    4 \\
    5 \\
    6 \\
    \end{bmatrix} + 
    \begin{bmatrix}
    10 \\
    20 \\
    30 \\
    \end{bmatrix}
    =
    \begin{bmatrix}
    14 \\
    25 \\
    36 \\
    \end{bmatrix} \text{, }
    \begin{bmatrix}
    4 \\
    5 \\
    6 \\
    \end{bmatrix} -
    \begin{bmatrix}
    10 \\
    20 \\
    30 \\
    \end{bmatrix}
    =
    \begin{bmatrix}
    -6 \\
    -15 \\
    -24 \\
    \end{bmatrix}
    $$
    - **Scalar-Vector Multiplication**: Scaling the vector without changing its direction.
    $$
    \lambda = 4 \text{, } w = \begin{bmatrix}
    9 \\
    4 \\
    1 \\
    \end{bmatrix} \text{, } \lambda w = \begin{bmatrix}
    36\\
    16\\
    4 \\
    \end{bmatrix}
    $$
    $$
    \lambda: \text{ scalar, w: vector}
    $$
    - **Vector Norms**:
        - **Manhattan Norm (L1 Norm)**: Sum of the absolute values of vector components.
        $$
        ||v||_1 = \Sigma^n_i|x_i| = |x_1| + |x_2| + ... + |x_n|
        $$
        - **Euclidean Norm (L2 Norm)**: Square root of the sum of the squares of vector components.
        $$
        ||v||_2 = \sqrt(\Sigma^n_i(x_i^2)) = \sqrt(x_1^2 + x_2^2 + x_3^2 + ... + x_i^2)
        $$
- **[[Dot Product]]**:
    
    - **Definition**: A scalar product or inner product calculated by multiplying corresponding elements of two vectors and summing them up.
    $$
    \delta = \sum_{i=1}^{n} a_i b_i
    $$
    $$
    u \cdot v = u^T v
    $$
    - **Properties**: Measures similarity or mapping between two vectors.
    $$
    a \cdot (b+c) = a^T(b+c) = a^Tb + a^Tc
    $$
    - **Geometric Interpretation**: Relates to the angle between vectors, with specific cases for acute, right, and obtuse angles.
    $$
    \alpha = cos(\theta_{v,w})\| \mathbf{v} \| \| \mathbf{w} \|
    $$
- **[[Cross Product]]**:
    
    - **Definition**: The vector cross product in ‚Ñù3 space results in a vector orthogonal to the two input vectors.
    $$
    x \times y = (x_2y_3 - x_3y_2, x_3y_1 - x_1y_3, x_1y_2 - x_2y_1)
    $$
    - **Properties**: Includes distributive, anti-commutative, and scalar multiplication rules.
	    $$
	    1 \text{. }x \times y = -y \times x
	    $$
	    $$
	    2 \text{. } x \times (y + z) = (x \times y) + (x \times z)
	    $$
	    $$
	    3 \text{. } (x + y) \times z = (x \times z) + (y \times z)
	    $$
	    $$
	    4 \text{. } c(x \times y) = (cx) \times y = x \times (cy)
		$$
		$$
	    5 \text{. } x \times 0 = 0 \times x = 0
	    $$
	     $$
	    6 \text{. } x \times x = 0
	    $$
    
- **[[Hadamard Product]]**
	![[hadamard_prodect.png]]
    - **Definition**: Element-wise multiplication of two vectors or matrices of the same dimension.
    - **Notation**: Denoted by ‚äô.
- **[[Orthogonal]] [[Vector]] [[Decomposition]]**:
    
    - **Concept**: Decomposing a vector into components parallel and orthogonal to a reference vector.
    - **Applications**: Relevant in the Gram-Schmidt process and QR decomposition.

## Í≥µÏßÄÏÇ¨Ìï≠
<br>



## ÏàòÏóÖ ÎÇ¥Ïö©


# Îã§Ïùå Ìï† Ïùº(After Actions)
## ÏûëÏóÖ (Tasks)

### Exercises:
1. Create visualizations for vectors and their operations using Matlab.
2. Implement functions to calculate vector norms, dot products, and perform vector decomposition.
3. Write code for converting vectors between row and column orientations without using built-in functions.

## Ï†ïÎ¶¨ (Summary)
[[Matlab]]
- **[[Summary]] of Key Points**:
    
    - **Vector Arithmetic**: Operations are performed element-wise.
    - **Dot Product**: Encodes the relationship between two vectors as a single number.
    - **Orthogonality**: Two vectors are orthogonal if their dot product is zero.
    - **Decomposition**: Involves dividing a vector into parallel and orthogonal components with respect to a reference vector.


